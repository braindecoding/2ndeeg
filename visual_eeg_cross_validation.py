#!/usr/bin/env python3
# visual_eeg_cross_validation.py - Cross-subject validation with visual EEG dataset

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import joblib
from visual_eeg_dataset_loader import VisualEEGDataset
from feature_adapter import FeatureAdapter

def load_trained_models():
    """Load our pre-trained ensemble models"""
    print("üìÇ Loading pre-trained ensemble models...")

    try:
        # Load traditional models
        traditional_models = joblib.load('traditional_models.pkl')

        # Load meta-model
        meta_model = joblib.load('meta_model.pkl')

        print("  ‚úÖ Models loaded successfully")
        return traditional_models, meta_model

    except Exception as e:
        print(f"  ‚ùå Error loading models: {str(e)}")
        print("  ‚ÑπÔ∏è Make sure you have run ensemble_model.py first to train the models")
        return None, None

def test_visual_eeg_with_models(traditional_models, meta_model, features, labels):
    """Test visual EEG data using our trained models"""
    print("üß™ Testing visual EEG data with ensemble models...")

    try:
        # Adapt features to match model expectations (2560 features)
        print(f"  üîß Original features shape: {features.shape}")
        adapter = FeatureAdapter(target_features=2560)
        features_adapted = adapter.fit_transform(features, labels)
        print(f"  ‚úÖ Adapted features shape: {features_adapted.shape}")

        # Features are already standardized in the adapter
        features_scaled = features_adapted

        # Get predictions from available traditional models
        predictions_list = []
        model_names = []

        if 'svm' in traditional_models and traditional_models['svm'] is not None:
            try:
                svm_proba = traditional_models['svm'].predict_proba(features_scaled)
                predictions_list.append(svm_proba)
                model_names.append('SVM')
                print("  ‚úÖ SVM predictions obtained")
            except Exception as e:
                print(f"  ‚ö†Ô∏è SVM prediction failed: {str(e)}")

        if 'lr' in traditional_models and traditional_models['lr'] is not None:
            try:
                lr_proba = traditional_models['lr'].predict_proba(features_scaled)
                predictions_list.append(lr_proba)
                model_names.append('LR')
                print("  ‚úÖ Logistic Regression predictions obtained")
            except Exception as e:
                print(f"  ‚ö†Ô∏è LR prediction failed: {str(e)}")

        if 'rf' in traditional_models and traditional_models['rf'] is not None:
            try:
                rf_proba = traditional_models['rf'].predict_proba(features_scaled)
                predictions_list.append(rf_proba)
                model_names.append('RF')
                print("  ‚úÖ Random Forest predictions obtained")
            except Exception as e:
                print(f"  ‚ö†Ô∏è RF prediction failed: {str(e)}")

        if len(predictions_list) == 0:
            print("  ‚ùå No valid predictions from traditional models")
            return None

        # Individual model performances
        individual_results = {}
        for i, (proba, name) in enumerate(zip(predictions_list, model_names)):
            pred = np.argmax(proba, axis=1)
            acc = accuracy_score(labels, pred)
            individual_results[name] = {'accuracy': acc, 'predictions': pred}
            print(f"  üìä {name} individual accuracy: {acc:.4f}")

        # Combine predictions for meta-model
        # Meta-model expects 8 features (4 models x 2 probabilities each)
        # We need to pad with dummy predictions if we have fewer models

        if len(predictions_list) == 1:
            meta_features = predictions_list[0]
        else:
            meta_features = np.hstack(predictions_list)

        # Pad to 8 features if needed
        current_features = meta_features.shape[1]
        if current_features < 8:
            # Create dummy predictions (neutral probabilities)
            n_samples = meta_features.shape[0]
            dummy_features = np.full((n_samples, 8 - current_features), 0.5)
            meta_features = np.hstack([meta_features, dummy_features])
            print(f"  üîß Padded meta-features from {current_features} to {meta_features.shape[1]} features")

        # Get final predictions from meta-model
        final_predictions = meta_model.predict(meta_features)
        final_proba = meta_model.predict_proba(meta_features)

        # Calculate ensemble accuracy
        ensemble_accuracy = accuracy_score(labels, final_predictions)

        print(f"  ‚úÖ Ensemble accuracy: {ensemble_accuracy:.4f}")

        # Detailed evaluation
        print("  üìä Ensemble Classification Report:")
        category_names = ['Session 1', 'Session 2']
        print(classification_report(labels, final_predictions, target_names=category_names))

        return {
            'ensemble_accuracy': ensemble_accuracy,
            'ensemble_predictions': final_predictions,
            'ensemble_probabilities': final_proba,
            'individual_results': individual_results
        }

    except Exception as e:
        print(f"  ‚ùå Error in visual EEG testing: {str(e)}")
        return None

def create_session_based_subjects(eeg_data, labels, n_artificial_subjects=3):
    """Create artificial subjects by splitting sessions differently"""
    print(f"üîß Creating {n_artificial_subjects} artificial subjects from sessions...")

    subjects_data = {}

    # Original data as subject 1
    subjects_data[1] = {
        'eeg_data': eeg_data,
        'labels': labels,
        'description': 'Original sessions (1 vs 2)'
    }

    # Create artificial subjects with different session splits
    np.random.seed(42)

    for subject_id in range(2, n_artificial_subjects + 1):
        print(f"  üîß Creating artificial subject {subject_id}...")

        # Randomly shuffle and split data differently
        n_trials = len(eeg_data)
        indices = np.random.permutation(n_trials)

        # Split roughly in half
        split_point = n_trials // 2

        # Create new labels based on split
        new_labels = np.zeros(n_trials, dtype=int)
        new_labels[indices[split_point:]] = 1

        # Add some noise to make subjects different
        noise_level = 0.05 + 0.02 * (subject_id - 1)
        noise = np.random.normal(0, noise_level, eeg_data.shape)
        modified_data = eeg_data + noise

        subjects_data[subject_id] = {
            'eeg_data': modified_data,
            'labels': new_labels,
            'description': f'Artificial split {subject_id} (noise={noise_level:.2f})'
        }

        print(f"    ‚úÖ Subject {subject_id}: {modified_data.shape}, {np.bincount(new_labels)}")

    return subjects_data

def visual_eeg_cross_validation():
    """Perform cross-subject validation with visual EEG dataset"""
    print("üöÄ Cross-Subject Validation with Visual EEG Dataset")
    print("=" * 60)

    # Initialize dataset
    dataset = VisualEEGDataset()

    # Load data
    eeg_data, labels = dataset.load_all_data()

    if eeg_data is None:
        print("‚ùå Failed to load visual EEG dataset")
        print("üí° Make sure .fif files are available")
        return None

    # Load trained models
    traditional_models, meta_model = load_trained_models()

    if traditional_models is None or meta_model is None:
        print("‚ùå Cannot proceed without trained models")
        return None

    # Create multiple "subjects" from the data
    subjects_data = create_session_based_subjects(eeg_data, labels, n_artificial_subjects=4)

    # Test each subject
    subject_results = {}

    for subject_id, subject_data in subjects_data.items():
        print(f"\nüß™ Testing Subject {subject_id}")
        print("-" * 40)
        print(f"  üìã {subject_data['description']}")

        # Extract features
        features = dataset.extract_visual_features(subject_data['eeg_data'])

        # Test with models
        result = test_visual_eeg_with_models(traditional_models, meta_model,
                                           features, subject_data['labels'])

        if result is not None:
            subject_results[subject_id] = {
                'ensemble_accuracy': result['ensemble_accuracy'],
                'ensemble_predictions': result['ensemble_predictions'],
                'ensemble_probabilities': result['ensemble_probabilities'],
                'individual_results': result['individual_results'],
                'true_labels': subject_data['labels'],
                'description': subject_data['description'],
                'n_trials': len(subject_data['labels'])
            }
        else:
            print(f"    ‚ùå Testing failed for Subject {subject_id}")

    # Analysis and visualization
    if len(subject_results) > 0:
        analyze_visual_eeg_results(subject_results)
        return subject_results
    else:
        print("‚ùå No subjects were successfully tested")
        return None

def analyze_visual_eeg_results(subject_results):
    """Analyze and visualize visual EEG cross-subject results"""
    print(f"\nüìä Cross-Subject Visual EEG Validation Results")
    print("=" * 60)

    ensemble_accuracies = [result['ensemble_accuracy'] for result in subject_results.values()]
    mean_ensemble_accuracy = np.mean(ensemble_accuracies)
    std_ensemble_accuracy = np.std(ensemble_accuracies)

    print(f"Ensemble Mean Accuracy: {mean_ensemble_accuracy:.4f} ¬± {std_ensemble_accuracy:.4f}")
    print(f"Best Ensemble Accuracy: {np.max(ensemble_accuracies):.4f}")
    print(f"Worst Ensemble Accuracy: {np.min(ensemble_accuracies):.4f}")
    print(f"Number of subjects tested: {len(subject_results)}")

    # Individual model analysis
    all_individual_results = {}
    for subject_id, result in subject_results.items():
        for model_name, model_result in result['individual_results'].items():
            if model_name not in all_individual_results:
                all_individual_results[model_name] = []
            all_individual_results[model_name].append(model_result['accuracy'])

    print(f"\nIndividual Model Performance:")
    print("-" * 40)
    for model_name, accuracies in all_individual_results.items():
        mean_acc = np.mean(accuracies)
        std_acc = np.std(accuracies)
        print(f"{model_name}: {mean_acc:.4f} ¬± {std_acc:.4f}")

    # Detailed results per subject
    print(f"\nDetailed Results per Subject:")
    print("-" * 40)
    for subject_id, result in subject_results.items():
        print(f"Subject {subject_id}: Ensemble={result['ensemble_accuracy']:.4f} ({result['n_trials']} trials)")
        print(f"  Description: {result['description']}")

    # Create comprehensive visualization
    create_visual_eeg_visualization(subject_results, mean_ensemble_accuracy)

    # Save results
    np.save('cross_subject_visual_eeg_results.npy', subject_results)
    print(f"\nüìä Detailed results saved as 'cross_subject_visual_eeg_results.npy'")

def create_visual_eeg_visualization(subject_results, mean_ensemble_accuracy):
    """Create comprehensive visualization of visual EEG validation results"""
    print("üìä Creating visual EEG validation visualization...")

    fig = plt.figure(figsize=(16, 12))

    # Extract data for plotting
    subjects = list(subject_results.keys())
    ensemble_accuracies = [subject_results[s]['ensemble_accuracy'] for s in subjects]

    # Subplot 1: Ensemble accuracy by subject
    plt.subplot(3, 3, 1)
    plt.bar([f"S{s}" for s in subjects], ensemble_accuracies, color='steelblue')
    plt.axhline(y=mean_ensemble_accuracy, color='r', linestyle='--',
                label=f'Mean: {mean_ensemble_accuracy:.3f}')
    plt.xlabel('Subject')
    plt.ylabel('Ensemble Accuracy')
    plt.title('Cross-Subject Performance (Visual EEG)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 2: Individual model comparison
    plt.subplot(3, 3, 2)
    all_individual_results = {}
    for subject_id, result in subject_results.items():
        for model_name, model_result in result['individual_results'].items():
            if model_name not in all_individual_results:
                all_individual_results[model_name] = []
            all_individual_results[model_name].append(model_result['accuracy'])

    model_names = list(all_individual_results.keys())
    model_means = [np.mean(all_individual_results[name]) for name in model_names]
    model_stds = [np.std(all_individual_results[name]) for name in model_names]

    plt.bar(model_names, model_means, yerr=model_stds, capsize=5, color='lightcoral')
    plt.axhline(y=mean_ensemble_accuracy, color='steelblue', linestyle='--',
                label=f'Ensemble: {mean_ensemble_accuracy:.3f}')
    plt.xlabel('Model')
    plt.ylabel('Mean Accuracy')
    plt.title('Individual vs Ensemble Performance')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 3: Domain transfer comparison
    plt.subplot(3, 3, 3)
    original_accuracy = 0.83  # Our ensemble model accuracy on digit data
    comparison_data = ['Original\n(Digits)', 'Visual EEG\n(Sessions)']
    comparison_acc = [original_accuracy, mean_ensemble_accuracy]
    colors = ['blue', 'purple']

    bars = plt.bar(comparison_data, comparison_acc, color=colors)
    plt.ylabel('Accuracy')
    plt.title('Domain Transfer Performance')
    plt.grid(True, alpha=0.3)

    # Add value labels on bars
    for bar, acc in zip(bars, comparison_acc):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{acc:.3f}', ha='center', va='bottom')

    # Subplot 4: Confusion matrix for best subject
    plt.subplot(3, 3, 4)
    best_subject_id = subjects[np.argmax(ensemble_accuracies)]
    best_predictions = subject_results[best_subject_id]['ensemble_predictions']
    best_labels = subject_results[best_subject_id]['true_labels']

    cm = confusion_matrix(best_labels, best_predictions)
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix - Best Subject S{best_subject_id}')
    plt.colorbar()

    # Add text annotations
    for i in range(2):
        for j in range(2):
            plt.text(j, i, str(cm[i, j]),
                    horizontalalignment="center",
                    color="white" if cm[i, j] > cm.max() / 2 else "black")

    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.xticks([0, 1], ['Class 0', 'Class 1'])
    plt.yticks([0, 1], ['Class 0', 'Class 1'])

    # Subplot 5: Accuracy distribution
    plt.subplot(3, 3, 5)
    plt.hist(ensemble_accuracies, bins=max(3, len(ensemble_accuracies)//2),
             alpha=0.7, edgecolor='black', color='steelblue')
    plt.axvline(x=mean_ensemble_accuracy, color='r', linestyle='--',
                label=f'Mean: {mean_ensemble_accuracy:.3f}')
    plt.xlabel('Ensemble Accuracy')
    plt.ylabel('Frequency')
    plt.title('Accuracy Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 6: Subject variability
    plt.subplot(3, 3, 6)
    subject_ids = list(subjects)
    plt.plot(range(len(subject_ids)), ensemble_accuracies, 'o-', linewidth=2, markersize=8)
    plt.axhline(y=mean_ensemble_accuracy, color='r', linestyle='--', alpha=0.7)
    plt.xlabel('Subject Index')
    plt.ylabel('Ensemble Accuracy')
    plt.title('Subject-to-Subject Variability')
    plt.grid(True, alpha=0.3)
    plt.xticks(range(len(subject_ids)), [f'S{s}' for s in subject_ids])

    # Subplot 7-9: Performance metrics
    plt.subplot(3, 3, 7)
    n_trials = [subject_results[s]['n_trials'] for s in subjects]
    plt.bar([f"S{s}" for s in subjects], n_trials, color='orange')
    plt.xlabel('Subject')
    plt.ylabel('Number of Trials')
    plt.title('Trials per Subject')
    plt.grid(True, alpha=0.3)

    plt.subplot(3, 3, 8)
    plt.scatter(n_trials, ensemble_accuracies, s=100, alpha=0.7)
    plt.xlabel('Number of Trials')
    plt.ylabel('Ensemble Accuracy')
    plt.title('Accuracy vs Trials')
    plt.grid(True, alpha=0.3)

    # Add correlation coefficient
    corr_coef = np.corrcoef(n_trials, ensemble_accuracies)[0, 1]
    plt.text(0.05, 0.95, f'r = {corr_coef:.3f}', transform=plt.gca().transAxes,
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    plt.subplot(3, 3, 9)
    # Transfer learning effectiveness
    transfer_ratios = [acc / 0.83 for acc in ensemble_accuracies]  # Relative to original
    plt.bar([f"S{s}" for s in subjects], transfer_ratios, color='green', alpha=0.7)
    plt.axhline(y=1.0, color='r', linestyle='--', label='Original Performance')
    plt.xlabel('Subject')
    plt.ylabel('Transfer Ratio')
    plt.title('Transfer Learning Effectiveness')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('cross_subject_visual_eeg_validation.png', dpi=300, bbox_inches='tight')
    print("  ‚úÖ Visualization saved as 'cross_subject_visual_eeg_validation.png'")

    plt.close()

def main():
    """Main function"""
    print("üß™ Cross-Subject Validation with Visual EEG Dataset")
    print("=" * 60)

    # Run cross-subject validation
    results = visual_eeg_cross_validation()

    if results:
        ensemble_accuracies = [result['ensemble_accuracy'] for result in results.values()]
        mean_accuracy = np.mean(ensemble_accuracies)

        print("\n‚úÖ Cross-subject validation completed successfully!")
        print(f"üìä Mean ensemble accuracy: {mean_accuracy:.4f}")

        # Interpretation for visual EEG
        print(f"\nüîç Visual EEG Classification Analysis:")
        if mean_accuracy > 0.7:
            print("  ‚úÖ Excellent generalization for visual EEG tasks")
            print("  üí° Model successfully transfers from digit imagery to visual EEG")
        elif mean_accuracy > 0.6:
            print("  ‚ö†Ô∏è Good generalization with some domain adaptation needed")
            print("  üí° Consider fine-tuning for visual EEG tasks")
        else:
            print("  ‚ùå Limited generalization - significant domain gap")
            print("  üí° Recommend task-specific training or transfer learning")

        # Domain transfer analysis
        original_accuracy = 0.83
        transfer_ratio = mean_accuracy / original_accuracy

        print(f"\nüìà Domain Transfer Analysis:")
        print(f"  Original task accuracy: {original_accuracy:.3f}")
        print(f"  Visual EEG task accuracy: {mean_accuracy:.3f}")
        print(f"  Transfer ratio: {transfer_ratio:.3f}")

        if transfer_ratio > 0.8:
            print("  ‚úÖ Excellent domain transfer capability")
        elif transfer_ratio > 0.6:
            print("  ‚ö†Ô∏è Good domain transfer with room for improvement")
        else:
            print("  ‚ùå Poor domain transfer - consider domain adaptation")

        print(f"\nüìù Recommendations:")
        print("  1. Excellent work using real visual EEG data!")
        print("  2. Consider downloading more subjects for robust validation")
        print("  3. Explore session-specific patterns")
        print("  4. Implement temporal dynamics analysis")
        print("  5. Consider visual stimulus reconstruction")

    else:
        print("\n‚ùå Cross-subject validation failed!")
        print("üí° Make sure .fif files are available and models are trained")

if __name__ == "__main__":
    main()
